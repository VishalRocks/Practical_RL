{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True \n",
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-30 13:31:54,975] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.full((n_states,n_actions),1./6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        a = np.random.choice([0,1,2,3,4,5])\n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        np.append(states,s)\n",
    "        np.append(actions,a)\n",
    "        total_reward += r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,a,r = generate_session()\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) is float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.85 s, sys: 0 ns, total: 1.85 s\n",
      "Wall time: 1.85 s\n",
      "mean reward = -773.78400\tthreshold = -785.0\n",
      "CPU times: user 1.8 s, sys: 0 ns, total: 1.8 s\n",
      "Wall time: 1.79 s\n",
      "mean reward = -779.14000\tthreshold = -785.0\n",
      "CPU times: user 1.8 s, sys: 0 ns, total: 1.8 s\n",
      "Wall time: 1.8 s\n",
      "mean reward = -760.31200\tthreshold = -776.0\n",
      "CPU times: user 1.76 s, sys: 0 ns, total: 1.76 s\n",
      "Wall time: 1.76 s\n",
      "mean reward = -752.61200\tthreshold = -776.0\n",
      "CPU times: user 1.77 s, sys: 0 ns, total: 1.77 s\n",
      "Wall time: 1.76 s\n",
      "mean reward = -764.78400\tthreshold = -776.0\n",
      "CPU times: user 1.92 s, sys: 0 ns, total: 1.92 s\n",
      "Wall time: 1.92 s\n",
      "mean reward = -774.90800\tthreshold = -785.0\n",
      "CPU times: user 2.23 s, sys: 0 ns, total: 2.23 s\n",
      "Wall time: 2.23 s\n",
      "mean reward = -767.55600\tthreshold = -785.0\n",
      "CPU times: user 1.96 s, sys: 0 ns, total: 1.96 s\n",
      "Wall time: 1.97 s\n",
      "mean reward = -764.06000\tthreshold = -776.0\n",
      "CPU times: user 1.76 s, sys: 0 ns, total: 1.76 s\n",
      "Wall time: 1.76 s\n",
      "mean reward = -773.52400\tthreshold = -785.0\n",
      "CPU times: user 1.74 s, sys: 0 ns, total: 1.74 s\n",
      "Wall time: 1.75 s\n",
      "mean reward = -770.27200\tthreshold = -794.0\n",
      "CPU times: user 1.91 s, sys: 0 ns, total: 1.91 s\n",
      "Wall time: 1.91 s\n",
      "mean reward = -762.92400\tthreshold = -794.0\n",
      "CPU times: user 2.18 s, sys: 0 ns, total: 2.18 s\n",
      "Wall time: 2.18 s\n",
      "mean reward = -760.93200\tthreshold = -776.0\n",
      "CPU times: user 2.62 s, sys: 0 ns, total: 2.62 s\n",
      "Wall time: 2.62 s\n",
      "mean reward = -784.93600\tthreshold = -794.0\n",
      "CPU times: user 1.86 s, sys: 0 ns, total: 1.86 s\n",
      "Wall time: 1.87 s\n",
      "mean reward = -761.90000\tthreshold = -776.0\n",
      "CPU times: user 1.84 s, sys: 0 ns, total: 1.84 s\n",
      "Wall time: 1.84 s\n",
      "mean reward = -771.87200\tthreshold = -776.0\n",
      "CPU times: user 1.74 s, sys: 0 ns, total: 1.74 s\n",
      "Wall time: 1.73 s\n",
      "mean reward = -760.39600\tthreshold = -780.5\n",
      "CPU times: user 1.77 s, sys: 0 ns, total: 1.77 s\n",
      "Wall time: 1.77 s\n",
      "mean reward = -775.68800\tthreshold = -785.0\n",
      "CPU times: user 1.76 s, sys: 0 ns, total: 1.76 s\n",
      "Wall time: 1.76 s\n",
      "mean reward = -765.07600\tthreshold = -785.0\n",
      "CPU times: user 2.6 s, sys: 0 ns, total: 2.6 s\n",
      "Wall time: 2.59 s\n",
      "mean reward = -760.67200\tthreshold = -776.0\n",
      "CPU times: user 2.28 s, sys: 0 ns, total: 2.28 s\n",
      "Wall time: 2.27 s\n",
      "mean reward = -759.37600\tthreshold = -776.0\n",
      "CPU times: user 1.84 s, sys: 0 ns, total: 1.84 s\n",
      "Wall time: 1.84 s\n",
      "mean reward = -767.01600\tthreshold = -776.0\n",
      "CPU times: user 1.71 s, sys: 0 ns, total: 1.71 s\n",
      "Wall time: 1.71 s\n",
      "mean reward = -748.72000\tthreshold = -776.0\n",
      "CPU times: user 2.04 s, sys: 0 ns, total: 2.04 s\n",
      "Wall time: 2.04 s\n",
      "mean reward = -775.86800\tthreshold = -785.0\n",
      "CPU times: user 2.13 s, sys: 0 ns, total: 2.13 s\n",
      "Wall time: 2.13 s\n",
      "mean reward = -768.38000\tthreshold = -776.0\n",
      "CPU times: user 1.95 s, sys: 0 ns, total: 1.95 s\n",
      "Wall time: 1.96 s\n",
      "mean reward = -761.20000\tthreshold = -776.0\n",
      "CPU times: user 1.98 s, sys: 0 ns, total: 1.98 s\n",
      "Wall time: 1.98 s\n",
      "mean reward = -765.88000\tthreshold = -785.0\n",
      "CPU times: user 2.33 s, sys: 0 ns, total: 2.33 s\n",
      "Wall time: 2.33 s\n",
      "mean reward = -764.57200\tthreshold = -776.0\n",
      "CPU times: user 2.57 s, sys: 0 ns, total: 2.57 s\n",
      "Wall time: 2.56 s\n",
      "mean reward = -758.46800\tthreshold = -785.0\n",
      "CPU times: user 3.8 s, sys: 0 ns, total: 3.8 s\n",
      "Wall time: 3.8 s\n",
      "mean reward = -777.68000\tthreshold = -785.0\n",
      "CPU times: user 2.09 s, sys: 0 ns, total: 2.09 s\n",
      "Wall time: 2.09 s\n",
      "mean reward = -770.55600\tthreshold = -794.0\n",
      "CPU times: user 1.95 s, sys: 0 ns, total: 1.95 s\n",
      "Wall time: 1.95 s\n",
      "mean reward = -762.15600\tthreshold = -776.0\n",
      "CPU times: user 2.11 s, sys: 10 ms, total: 2.12 s\n",
      "Wall time: 2.12 s\n",
      "mean reward = -779.14000\tthreshold = -785.0\n",
      "CPU times: user 1.88 s, sys: 0 ns, total: 1.88 s\n",
      "Wall time: 1.88 s\n",
      "mean reward = -759.16800\tthreshold = -776.0\n",
      "CPU times: user 1.81 s, sys: 0 ns, total: 1.81 s\n",
      "Wall time: 1.81 s\n",
      "mean reward = -765.98400\tthreshold = -776.0\n",
      "CPU times: user 1.81 s, sys: 0 ns, total: 1.81 s\n",
      "Wall time: 1.81 s\n",
      "mean reward = -763.36400\tthreshold = -776.0\n",
      "CPU times: user 1.81 s, sys: 0 ns, total: 1.81 s\n",
      "Wall time: 1.81 s\n",
      "mean reward = -759.86000\tthreshold = -776.0\n",
      "CPU times: user 1.84 s, sys: 0 ns, total: 1.84 s\n",
      "Wall time: 1.84 s\n",
      "mean reward = -775.46000\tthreshold = -785.0\n",
      "CPU times: user 2.04 s, sys: 0 ns, total: 2.04 s\n",
      "Wall time: 2.06 s\n",
      "mean reward = -776.84000\tthreshold = -794.0\n",
      "CPU times: user 2.94 s, sys: 20 ms, total: 2.96 s\n",
      "Wall time: 2.96 s\n",
      "mean reward = -777.72800\tthreshold = -785.0\n",
      "CPU times: user 1.95 s, sys: 0 ns, total: 1.95 s\n",
      "Wall time: 1.94 s\n",
      "mean reward = -769.61600\tthreshold = -785.0\n",
      "CPU times: user 2.07 s, sys: 0 ns, total: 2.07 s\n",
      "Wall time: 2.07 s\n",
      "mean reward = -768.03600\tthreshold = -776.0\n",
      "CPU times: user 2.03 s, sys: 0 ns, total: 2.03 s\n",
      "Wall time: 2.03 s\n",
      "mean reward = -768.20400\tthreshold = -780.5\n",
      "CPU times: user 1.74 s, sys: 0 ns, total: 1.74 s\n",
      "Wall time: 1.73 s\n",
      "mean reward = -764.13600\tthreshold = -785.0\n",
      "CPU times: user 2.11 s, sys: 0 ns, total: 2.11 s\n",
      "Wall time: 2.11 s\n",
      "mean reward = -770.75200\tthreshold = -780.5\n",
      "CPU times: user 2.22 s, sys: 0 ns, total: 2.22 s\n",
      "Wall time: 2.23 s\n",
      "mean reward = -787.59600\tthreshold = -794.0\n",
      "CPU times: user 2.66 s, sys: 0 ns, total: 2.66 s\n",
      "Wall time: 2.65 s\n",
      "mean reward = -772.75200\tthreshold = -785.0\n",
      "CPU times: user 2.19 s, sys: 0 ns, total: 2.19 s\n",
      "Wall time: 2.19 s\n",
      "mean reward = -772.06400\tthreshold = -785.0\n",
      "CPU times: user 2.17 s, sys: 0 ns, total: 2.17 s\n",
      "Wall time: 2.17 s\n",
      "mean reward = -765.07600\tthreshold = -776.0\n",
      "CPU times: user 1.93 s, sys: 10 ms, total: 1.94 s\n",
      "Wall time: 1.95 s\n",
      "mean reward = -774.95600\tthreshold = -789.5\n",
      "CPU times: user 1.82 s, sys: 0 ns, total: 1.82 s\n",
      "Wall time: 1.82 s\n",
      "mean reward = -765.14000\tthreshold = -776.0\n",
      "CPU times: user 1.74 s, sys: 0 ns, total: 1.74 s\n",
      "Wall time: 1.74 s\n",
      "mean reward = -773.73600\tthreshold = -785.0\n",
      "CPU times: user 2.2 s, sys: 0 ns, total: 2.2 s\n",
      "Wall time: 2.2 s\n",
      "mean reward = -765.27600\tthreshold = -776.0\n",
      "CPU times: user 2.18 s, sys: 0 ns, total: 2.18 s\n",
      "Wall time: 2.18 s\n",
      "mean reward = -769.71600\tthreshold = -785.0\n",
      "CPU times: user 3.06 s, sys: 20 ms, total: 3.08 s\n",
      "Wall time: 3.08 s\n",
      "mean reward = -768.86800\tthreshold = -785.0\n",
      "CPU times: user 2.19 s, sys: 10 ms, total: 2.2 s\n",
      "Wall time: 2.19 s\n",
      "mean reward = -765.96800\tthreshold = -785.0\n",
      "CPU times: user 1.91 s, sys: 10 ms, total: 1.92 s\n",
      "Wall time: 1.91 s\n",
      "mean reward = -764.11600\tthreshold = -776.0\n",
      "CPU times: user 1.85 s, sys: 0 ns, total: 1.85 s\n",
      "Wall time: 1.85 s\n",
      "mean reward = -764.62000\tthreshold = -776.0\n",
      "CPU times: user 1.91 s, sys: 10 ms, total: 1.92 s\n",
      "Wall time: 1.91 s\n",
      "mean reward = -765.51200\tthreshold = -785.0\n",
      "CPU times: user 2.02 s, sys: 10 ms, total: 2.03 s\n",
      "Wall time: 2.03 s\n",
      "mean reward = -763.88000\tthreshold = -785.0\n",
      "CPU times: user 2.36 s, sys: 0 ns, total: 2.36 s\n",
      "Wall time: 2.35 s\n",
      "mean reward = -775.22400\tthreshold = -785.0\n",
      "CPU times: user 2.52 s, sys: 0 ns, total: 2.52 s\n",
      "Wall time: 2.52 s\n",
      "mean reward = -753.98800\tthreshold = -776.0\n",
      "CPU times: user 2.13 s, sys: 0 ns, total: 2.13 s\n",
      "Wall time: 2.14 s\n",
      "mean reward = -776.02000\tthreshold = -782.0\n",
      "CPU times: user 1.99 s, sys: 0 ns, total: 1.99 s\n",
      "Wall time: 1.99 s\n",
      "mean reward = -779.76800\tthreshold = -794.0\n",
      "CPU times: user 2.62 s, sys: 10 ms, total: 2.63 s\n",
      "Wall time: 2.62 s\n",
      "mean reward = -755.59200\tthreshold = -776.0\n",
      "CPU times: user 2.92 s, sys: 10 ms, total: 2.93 s\n",
      "Wall time: 2.93 s\n",
      "mean reward = -773.73200\tthreshold = -785.0\n",
      "CPU times: user 2.52 s, sys: 0 ns, total: 2.52 s\n",
      "Wall time: 2.53 s\n",
      "mean reward = -759.38400\tthreshold = -776.0\n",
      "CPU times: user 2.84 s, sys: 0 ns, total: 2.84 s\n",
      "Wall time: 2.84 s\n",
      "mean reward = -778.14400\tthreshold = -785.0\n",
      "CPU times: user 3.19 s, sys: 10 ms, total: 3.2 s\n",
      "Wall time: 3.2 s\n",
      "mean reward = -769.35600\tthreshold = -785.0\n",
      "CPU times: user 2.69 s, sys: 0 ns, total: 2.69 s\n",
      "Wall time: 2.69 s\n",
      "mean reward = -781.62800\tthreshold = -794.0\n",
      "CPU times: user 3 s, sys: 0 ns, total: 3 s\n",
      "Wall time: 3.01 s\n",
      "mean reward = -760.32800\tthreshold = -767.0\n",
      "CPU times: user 3 s, sys: 20 ms, total: 3.02 s\n",
      "Wall time: 3.02 s\n",
      "mean reward = -754.73600\tthreshold = -776.0\n",
      "CPU times: user 3.01 s, sys: 0 ns, total: 3.01 s\n",
      "Wall time: 3.01 s\n",
      "mean reward = -766.07200\tthreshold = -776.0\n",
      "CPU times: user 3.31 s, sys: 10 ms, total: 3.32 s\n",
      "Wall time: 3.31 s\n",
      "mean reward = -754.89600\tthreshold = -776.0\n",
      "CPU times: user 2.5 s, sys: 0 ns, total: 2.5 s\n",
      "Wall time: 2.5 s\n",
      "mean reward = -766.48800\tthreshold = -785.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.59 s, sys: 0 ns, total: 2.59 s\n",
      "Wall time: 2.61 s\n",
      "mean reward = -751.78000\tthreshold = -776.0\n",
      "CPU times: user 2.76 s, sys: 0 ns, total: 2.76 s\n",
      "Wall time: 2.76 s\n",
      "mean reward = -771.38400\tthreshold = -785.0\n",
      "CPU times: user 2.22 s, sys: 0 ns, total: 2.22 s\n",
      "Wall time: 2.21 s\n",
      "mean reward = -771.52000\tthreshold = -785.0\n",
      "CPU times: user 2.27 s, sys: 0 ns, total: 2.27 s\n",
      "Wall time: 2.27 s\n",
      "mean reward = -767.60000\tthreshold = -785.0\n",
      "CPU times: user 2.23 s, sys: 0 ns, total: 2.23 s\n",
      "Wall time: 2.23 s\n",
      "mean reward = -772.47200\tthreshold = -785.0\n",
      "CPU times: user 1.84 s, sys: 0 ns, total: 1.84 s\n",
      "Wall time: 1.84 s\n",
      "mean reward = -772.10400\tthreshold = -776.0\n",
      "CPU times: user 1.77 s, sys: 0 ns, total: 1.77 s\n",
      "Wall time: 1.77 s\n",
      "mean reward = -771.58000\tthreshold = -776.0\n",
      "CPU times: user 2.12 s, sys: 0 ns, total: 2.12 s\n",
      "Wall time: 2.12 s\n",
      "mean reward = -769.90800\tthreshold = -785.0\n",
      "CPU times: user 1.79 s, sys: 0 ns, total: 1.79 s\n",
      "Wall time: 1.78 s\n",
      "mean reward = -774.02400\tthreshold = -776.0\n",
      "CPU times: user 1.72 s, sys: 0 ns, total: 1.72 s\n",
      "Wall time: 1.72 s\n",
      "mean reward = -763.09200\tthreshold = -776.0\n",
      "CPU times: user 1.76 s, sys: 0 ns, total: 1.76 s\n",
      "Wall time: 1.76 s\n",
      "mean reward = -775.21600\tthreshold = -785.0\n",
      "CPU times: user 1.8 s, sys: 0 ns, total: 1.8 s\n",
      "Wall time: 1.8 s\n",
      "mean reward = -777.02800\tthreshold = -785.0\n",
      "CPU times: user 1.83 s, sys: 0 ns, total: 1.83 s\n",
      "Wall time: 1.83 s\n",
      "mean reward = -758.74400\tthreshold = -785.0\n",
      "CPU times: user 2.17 s, sys: 0 ns, total: 2.17 s\n",
      "Wall time: 2.17 s\n",
      "mean reward = -779.00000\tthreshold = -785.0\n",
      "CPU times: user 2.23 s, sys: 0 ns, total: 2.23 s\n",
      "Wall time: 2.23 s\n",
      "mean reward = -769.19600\tthreshold = -794.0\n",
      "CPU times: user 2.13 s, sys: 10 ms, total: 2.14 s\n",
      "Wall time: 2.14 s\n",
      "mean reward = -764.16000\tthreshold = -785.0\n",
      "CPU times: user 2.02 s, sys: 10 ms, total: 2.03 s\n",
      "Wall time: 2.04 s\n",
      "mean reward = -765.90400\tthreshold = -776.0\n",
      "CPU times: user 1.93 s, sys: 0 ns, total: 1.93 s\n",
      "Wall time: 1.93 s\n",
      "mean reward = -757.56000\tthreshold = -785.0\n",
      "CPU times: user 2.1 s, sys: 10 ms, total: 2.11 s\n",
      "Wall time: 2.1 s\n",
      "mean reward = -776.74800\tthreshold = -785.0\n",
      "CPU times: user 2.16 s, sys: 0 ns, total: 2.16 s\n",
      "Wall time: 2.16 s\n",
      "mean reward = -770.96400\tthreshold = -785.0\n",
      "CPU times: user 2.72 s, sys: 20 ms, total: 2.74 s\n",
      "Wall time: 2.74 s\n",
      "mean reward = -769.58000\tthreshold = -785.0\n",
      "CPU times: user 2.21 s, sys: 0 ns, total: 2.21 s\n",
      "Wall time: 2.2 s\n",
      "mean reward = -762.48400\tthreshold = -771.5\n",
      "CPU times: user 1.78 s, sys: 10 ms, total: 1.79 s\n",
      "Wall time: 1.79 s\n",
      "mean reward = -775.42400\tthreshold = -785.0\n",
      "CPU times: user 1.77 s, sys: 0 ns, total: 1.77 s\n",
      "Wall time: 1.77 s\n",
      "mean reward = -767.29600\tthreshold = -785.0\n",
      "CPU times: user 1.72 s, sys: 0 ns, total: 1.72 s\n",
      "Wall time: 1.72 s\n",
      "mean reward = -766.20000\tthreshold = -785.0\n",
      "CPU times: user 1.79 s, sys: 0 ns, total: 1.79 s\n",
      "Wall time: 1.79 s\n",
      "mean reward = -769.71200\tthreshold = -780.5\n"
     ]
    }
   ],
   "source": [
    "n_samples = 250  #sample this many samples\n",
    "percentile = 50  #take this percent of session with highest rewards\n",
    "smoothing = 0.1  #add this thing to all counts for stability\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    %time sessions = [generate_session() for j in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "    \n",
    "    threshold = np.percentile(batch_rewards,50)\n",
    "    \n",
    "    elite_states = [batch_states[i] for i in range(n_samples) if batch_rewards[i]>=threshold]\n",
    "    elite_actions = [batch_actions[i] for i in range(n_samples) if batch_rewards[i]>=threshold]\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #hint on task above: use np.percentile and numpy-style indexing\n",
    "    \n",
    "    #count actions from elite states\n",
    "    elite_counts = np.zeros_like(policy)+smoothing\n",
    "    \n",
    "    for i in range(len(elite_states)):\n",
    "        elite_counts[elite_states[i]][elite_actions[i]]+=1\n",
    "\n",
    "    policy = normalize(elite_counts,axis=1)\n",
    "    \n",
    "    \n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) crossentropy method\n",
    "\n",
    "In this section we will train a neural network policy for continuous action space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-30 13:38:39,877] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f79f68b22e8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7a00091cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0] \n",
    "        a = np.random.choice([0,1],p=probs)\n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward = 17.49000\tthreshold = 18.0\n",
      "mean reward = 18.07000\tthreshold = 18.0\n",
      "mean reward = 19.26000\tthreshold = 20.0\n",
      "mean reward = 19.44000\tthreshold = 22.0\n",
      "mean reward = 16.67000\tthreshold = 18.3\n",
      "mean reward = 17.42000\tthreshold = 18.0\n",
      "mean reward = 17.57000\tthreshold = 19.0\n",
      "mean reward = 18.62000\tthreshold = 21.0\n",
      "mean reward = 16.65000\tthreshold = 18.0\n",
      "mean reward = 18.20000\tthreshold = 20.0\n",
      "mean reward = 17.22000\tthreshold = 18.3\n",
      "mean reward = 18.01000\tthreshold = 19.0\n",
      "mean reward = 18.31000\tthreshold = 21.0\n",
      "mean reward = 18.23000\tthreshold = 20.0\n",
      "mean reward = 17.93000\tthreshold = 19.3\n",
      "mean reward = 18.28000\tthreshold = 19.3\n",
      "mean reward = 16.20000\tthreshold = 18.3\n",
      "mean reward = 17.85000\tthreshold = 19.0\n",
      "mean reward = 16.60000\tthreshold = 18.0\n",
      "mean reward = 17.55000\tthreshold = 19.0\n",
      "mean reward = 17.81000\tthreshold = 20.0\n",
      "mean reward = 17.78000\tthreshold = 19.0\n",
      "mean reward = 19.20000\tthreshold = 20.3\n",
      "mean reward = 17.80000\tthreshold = 20.0\n",
      "mean reward = 18.84000\tthreshold = 20.0\n",
      "mean reward = 17.51000\tthreshold = 19.0\n",
      "mean reward = 17.56000\tthreshold = 19.0\n",
      "mean reward = 18.03000\tthreshold = 20.0\n",
      "mean reward = 18.09000\tthreshold = 21.0\n",
      "mean reward = 17.34000\tthreshold = 19.0\n",
      "mean reward = 18.18000\tthreshold = 19.3\n",
      "mean reward = 16.94000\tthreshold = 19.0\n",
      "mean reward = 17.53000\tthreshold = 18.0\n",
      "mean reward = 18.79000\tthreshold = 22.0\n",
      "mean reward = 18.44000\tthreshold = 20.3\n",
      "mean reward = 17.59000\tthreshold = 20.3\n",
      "mean reward = 20.01000\tthreshold = 23.0\n",
      "mean reward = 16.54000\tthreshold = 18.0\n",
      "mean reward = 18.59000\tthreshold = 19.3\n",
      "mean reward = 16.91000\tthreshold = 19.0\n",
      "mean reward = 16.63000\tthreshold = 18.0\n",
      "mean reward = 17.75000\tthreshold = 19.0\n",
      "mean reward = 17.31000\tthreshold = 18.0\n",
      "mean reward = 18.93000\tthreshold = 20.3\n",
      "mean reward = 16.93000\tthreshold = 18.0\n",
      "mean reward = 18.41000\tthreshold = 19.3\n",
      "mean reward = 18.65000\tthreshold = 19.0\n",
      "mean reward = 17.39000\tthreshold = 18.0\n",
      "mean reward = 16.28000\tthreshold = 17.0\n",
      "mean reward = 15.15000\tthreshold = 16.0\n",
      "mean reward = 18.28000\tthreshold = 19.0\n",
      "mean reward = 17.26000\tthreshold = 19.3\n",
      "mean reward = 18.95000\tthreshold = 21.3\n",
      "mean reward = 16.76000\tthreshold = 18.0\n",
      "mean reward = 19.62000\tthreshold = 21.0\n",
      "mean reward = 17.92000\tthreshold = 20.0\n",
      "mean reward = 18.38000\tthreshold = 22.0\n",
      "mean reward = 16.45000\tthreshold = 18.0\n",
      "mean reward = 17.70000\tthreshold = 19.0\n",
      "mean reward = 18.67000\tthreshold = 21.0\n",
      "mean reward = 18.53000\tthreshold = 21.3\n",
      "mean reward = 20.15000\tthreshold = 22.3\n",
      "mean reward = 16.95000\tthreshold = 17.0\n",
      "mean reward = 16.33000\tthreshold = 17.3\n",
      "mean reward = 18.53000\tthreshold = 19.3\n",
      "mean reward = 16.54000\tthreshold = 19.0\n",
      "mean reward = 18.88000\tthreshold = 22.0\n",
      "mean reward = 17.31000\tthreshold = 18.0\n",
      "mean reward = 18.34000\tthreshold = 20.0\n",
      "mean reward = 19.58000\tthreshold = 21.3\n",
      "mean reward = 17.62000\tthreshold = 20.0\n",
      "mean reward = 18.05000\tthreshold = 19.0\n",
      "mean reward = 19.01000\tthreshold = 23.0\n",
      "mean reward = 17.23000\tthreshold = 19.0\n",
      "mean reward = 18.66000\tthreshold = 21.3\n",
      "mean reward = 17.42000\tthreshold = 19.0\n",
      "mean reward = 18.00000\tthreshold = 21.3\n",
      "mean reward = 17.75000\tthreshold = 20.0\n",
      "mean reward = 17.00000\tthreshold = 19.0\n",
      "mean reward = 18.96000\tthreshold = 22.0\n",
      "mean reward = 19.35000\tthreshold = 22.0\n",
      "mean reward = 17.98000\tthreshold = 20.3\n",
      "mean reward = 19.78000\tthreshold = 21.3\n",
      "mean reward = 16.89000\tthreshold = 18.0\n",
      "mean reward = 18.39000\tthreshold = 19.0\n",
      "mean reward = 17.36000\tthreshold = 18.0\n",
      "mean reward = 17.74000\tthreshold = 19.0\n",
      "mean reward = 17.02000\tthreshold = 19.0\n",
      "mean reward = 18.76000\tthreshold = 21.0\n",
      "mean reward = 17.06000\tthreshold = 17.0\n",
      "mean reward = 18.02000\tthreshold = 17.3\n",
      "mean reward = 19.52000\tthreshold = 21.0\n",
      "mean reward = 17.28000\tthreshold = 19.0\n",
      "mean reward = 19.26000\tthreshold = 20.0\n",
      "mean reward = 18.53000\tthreshold = 20.6\n",
      "mean reward = 18.55000\tthreshold = 18.0\n",
      "mean reward = 17.82000\tthreshold = 20.0\n",
      "mean reward = 17.84000\tthreshold = 20.0\n",
      "mean reward = 17.16000\tthreshold = 17.0\n",
      "mean reward = 17.60000\tthreshold = 19.3\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "percentile = 70\n",
    "smoothing = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    #generate new sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "\n",
    "    threshold = np.percentile(batch_rewards,percentile)\n",
    "    \n",
    "    elite_states = [batch_states[i] for i in range(n_samples) if batch_rewards[i]>=threshold]\n",
    "    elite_actions = [batch_actions[i] for i in range(n_samples) if batch_rewards[i]>=threshold]\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #elite_states: a list of states from top games\n",
    "    #elite_actions: a list of actions from top games\n",
    "    \n",
    "    policy = agent.predict_proba(elite_states)\n",
    "\n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-30 17:47:46,204] Making new env: CartPole-v0\n",
      "[2017-07-30 17:47:46,217] Creating monitor directory videos\n",
      "[2017-07-30 17:47:46,244] Starting new video recorder writing to /notebooks/week1/videos/openaigym.video.0.25.video000000.mp4\n",
      "[2017-07-30 17:47:47,337] Starting new video recorder writing to /notebooks/week1/videos/openaigym.video.0.25.video000001.mp4\n",
      "[2017-07-30 17:47:47,723] Starting new video recorder writing to /notebooks/week1/videos/openaigym.video.0.25.video000008.mp4\n",
      "[2017-07-30 17:47:48,640] Starting new video recorder writing to /notebooks/week1/videos/openaigym.video.0.25.video000027.mp4\n",
      "[2017-07-30 17:47:49,144] Starting new video recorder writing to /notebooks/week1/videos/openaigym.video.0.25.video000064.mp4\n",
      "[2017-07-30 17:47:49,542] Finished writing results. You can upload them to the scoreboard via gym.upload('/notebooks/week1/videos')\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.25.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part I\n",
    "\n",
    "### Tabular correntropy method\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from -10k to aroung -500 score (+- 500) and stays there. This is in part because taxi-v2 has some hard-coded randomness in the environment. Other reason is that the percentile was chosen poorly.\n",
    "\n",
    "### Tasks\n",
    "- __1.1__ (1 pt) Modify the tabular CEM (CrossEntropyMethod) code to plot distribution of rewards and threshold on each tick.\n",
    "- __1.2__ (2 pts) Find out how the algorithm performance changes if you change different percentile and different n_samples.\n",
    "\n",
    "```<YOUR ANSWER>```\n",
    "\n",
    "\n",
    "- __1.3__ (2 pts) Tune the algorithm to end up with positive average score.\n",
    "- __1.4 bonus__ (1 pt) Try to achieve a distribution where 25% or more samples score above +9.0\n",
    "- __1.5 bonus__ (2 pts) Solve and upload [Taxi-v1](https://gym.openai.com/envs/Taxi-v1) to the openai gym.\n",
    "\n",
    "It's okay to modify the existing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part II\n",
    "\n",
    "### Deep crossentropy method\n",
    "\n",
    "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to upload the result and get to something harder.\n",
    "\n",
    "* if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* __2.1__ Go to the [gym site](http://gym.openai.com/), register and obtain __api key__.\n",
    "* __2.2__ (1 pt) Upload your result to gym via gym.upload (see Results tab above, the line you need is commented)\n",
    "* __2.3__ (3 pts) Pick one of environments: MountainCar-v0 or LunarLander-v2 (or both) and solve it.\n",
    "  * For MountainCar, learn to finish it in __less than 180 steps__\n",
    "  * For LunarLander, learn to get reward of __at least +50__\n",
    "  * See the tips section below, it's kinda important.\n",
    "  \n",
    "  \n",
    "* __2.4__ (1+ pt) Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
    "  \n",
    "  \n",
    "### Tips\n",
    "* Gym page: [mountaincar](https://gym.openai.com/envs/MountainCar-v0), [lunarlander](https://gym.openai.com/envs/LunarLander-v2)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "* __Please upload the results to openai gym and send links to all submissions in the e-mail__\n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "* __2.5 bonus__ Try to find a network architecture and training params that solve __both__ environments above (_Points depend on implementation_)\n",
    "\n",
    "* __2.6 bonus__ Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * [MountainCarContinuous-v0](https://gym.openai.com/envs/MountainCarContinuous-v0), [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2) (4+ points if it works)\n",
    "  \n",
    "* __2.7 bonus__ Use any deep learning framework of your choice to implement policy-gradient (see lectures) on any of those envs (4 +1 per env):\n",
    "  * CartPole-v0\n",
    "  * MountainCar-v0\n",
    "  * LunarLander-v2\n",
    "  * See __tips on policy gradient__ below.\n",
    "  \n",
    "\n",
    "* __2.8 bonus__ take your favorite deep learning framework and try to get above random in [Atari Breakout](https://gym.openai.com/envs/Breakout-v0) with crossentropy method over a convolutional network.\n",
    "  * Expect at least +10 points if you get this up and running, no deadlines apply ! \n",
    "  * __See tips below on where to start, they're cruicially important__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on policy gradient\n",
    "\n",
    "* The loss function is very similar to crossentropy method. You can get away with using rewards as  __sample_weights__.\n",
    "* If your algorithm converges to a poor strategy, try regularizing with entropy or just somehow prevent agent from picking actions deterministically (e.g. when probs = 0,0,1,0,0)\n",
    "* We will use `lasagne` later in the course so you can try to [learn it](http://lasagne.readthedocs.io/en/latest/user/tutorial.html).\n",
    "* If you don't want to mess with theano just yet, try [keras](https://keras.io/getting-started/sequential-model-guide/) or [mxnet](http://mxnet.io/tutorials/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tips on atari breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's all the pre-processing and tuning done for you in the code below\n",
    "* Once you got it working, it's probably a good idea to pre-train with autoencoder or something\n",
    "* We use last 4 frames as observations to account for ball velocity\n",
    "* The code below requires ```pip install Image``` and ```pip install gym[atari]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from breakout import make_breakout\n",
    "\n",
    "env = make_breakout()\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the initial state\n",
    "s = env.reset()\n",
    "print (s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plot first observation. Only one frame\n",
    "plt.imshow(s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next frame\n",
    "new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after 10 frames\n",
    "for _ in range(10):\n",
    "    new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "< tons of your code here or elsewhere >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
